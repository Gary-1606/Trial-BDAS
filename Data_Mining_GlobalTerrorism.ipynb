{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Data_Mining_GlobalTerrorism').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Complete_dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Date: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Province: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Attack: string (nullable = true)\n",
      " |-- Target: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Individual: integer (nullable = true)\n",
      " |-- Weapon: string (nullable = true)\n",
      " |-- Ishostkid: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'Date',\n",
       " 'Country',\n",
       " 'Region',\n",
       " 'Province',\n",
       " 'City',\n",
       " 'Attack',\n",
       " 'Target',\n",
       " 'Nationality',\n",
       " 'Group',\n",
       " 'Individual',\n",
       " 'Weapon',\n",
       " 'Ishostkid']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('Final1').getOrCreate()\n",
    "# Print data columns.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation - encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing\n",
    "\n",
    "Country_indexer = StringIndexer(inputCol='Country',outputCol='CountryIndex')\n",
    "Region_indexer = StringIndexer(inputCol='Region',outputCol='RegionIndex')\n",
    "Province_indexer = StringIndexer(inputCol='Province',outputCol='ProvinceIndex')\n",
    "City_indexer = StringIndexer(inputCol='City',outputCol='CityIndex')\n",
    "Target_indexer = StringIndexer(inputCol='Target',outputCol='TargetIndex')\n",
    "Nationality_indexer = StringIndexer(inputCol='Nationality',outputCol='NationalityIndex')\n",
    "Group_indexer = StringIndexer(inputCol='Group',outputCol='GroupIndex')\n",
    "Weapon_indexer = StringIndexer(inputCol='Weapon',outputCol='WeaponIndex')\n",
    "Attack_indexer = StringIndexer(inputCol='Attack',outputCol='label')\n",
    "\n",
    "#encoding\n",
    "\n",
    "Country_encoder = OneHotEncoder(inputCol='CountryIndex',outputCol='CountryVec')\n",
    "Region_encoder = OneHotEncoder(inputCol='RegionIndex',outputCol='RegionVec')\n",
    "Province_encoder = OneHotEncoder(inputCol='ProvinceIndex',outputCol='ProvinceVec')\n",
    "City_encoder = OneHotEncoder(inputCol='CityIndex',outputCol='CityVec')\n",
    "Target_encoder = OneHotEncoder(inputCol='TargetIndex',outputCol='TargetVec')\n",
    "Nationality_encoder = OneHotEncoder(inputCol='NationalityIndex',outputCol='NationalityVec')\n",
    "Group_encoder = OneHotEncoder(inputCol='GroupIndex',outputCol='GroupVec')\n",
    "Weapon_encoder = OneHotEncoder(inputCol='WeaponIndex',outputCol='WeaponVec')\n",
    "\n",
    "label_encoder = OneHotEncoder(inputCol='label',outputCol='label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can assemble all of this as one vector in the features column. \n",
    "assembler = VectorAssembler(inputCols=['Year','CountryVec',\n",
    " 'RegionVec',\n",
    " 'ProvinceVec',\n",
    " 'CityVec',\n",
    " 'TargetVec',\n",
    " 'NationalityVec',\n",
    " 'GroupVec',\n",
    " 'WeaponVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[Country_indexer,Region_indexer,Province_indexer,City_indexer,Target_indexer,\n",
    "                            Nationality_indexer,Group_indexer,Weapon_indexer,Attack_indexer,Country_encoder,Region_encoder,\n",
    "                            Province_encoder,City_encoder,Target_encoder,Nationality_encoder,Group_encoder,Weapon_encoder,assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, label: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(df)\n",
    "pipe_df = pipeline_model.transform(df)\n",
    "pipe_df = pipe_df.select('label','features')\n",
    "pipe_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 78380\n",
      "Test Dataset Count: 33476\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = pipe_df.randomSplit([0.7,0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler, Normalizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train_data)\n",
    "predictions = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5237483570319035\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.4582433811544721\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=0.5, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test_data)\n",
    "#predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "#print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.464374650008304\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test_data)\n",
    "#predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "#print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 89310\n",
      "Test Dataset Count: 22546\n",
      "Test set accuracy = 0.8096336378958574\n"
     ]
    }
   ],
   "source": [
    "##Logistics Regression##\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.8,0.2])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "# Instantiate the model.\n",
    "lr_model = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Fit the model.\n",
    "lr_model = lr_model.fit(train_data)\n",
    "\n",
    "# And evaluate the model using the test data.\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 78247\n",
      "Test Dataset Count: 33609\n",
      "Test set accuracy = 0.8121931625457467\n"
     ]
    }
   ],
   "source": [
    "##Logistics Regression##\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.7,0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "# Instantiate the model.\n",
    "lr_model = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Fit the model.\n",
    "lr_model = lr_model.fit(train_data)\n",
    "\n",
    "# And evaluate the model using the test data.\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.7437130726522372\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# generate the train/test split.\n",
    "(train_data, test_data) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=5, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train_data)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test_data)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
